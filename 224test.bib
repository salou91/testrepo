@BOOK{Alemi2019,
  title     = {The amazing journey of reason: From {DNA} to artificial
               intelligence},
  author    = {Alemi, Mario},
  publisher = {Springer Nature},
  location  = {Cham, Switzerland},
  edition   = {1},
  date      = {2019-12-07},
  pagetotal = {113},
  isbn      = {9783030259617,9783030259624},
  abstract  = {The Amazing Journey analyzes the latest results in chemistry,
               biology, neuroscience, anthropology and sociology under the light
               of the evolution of intelligence, seen as the ability of
               processing information.},
  series    = {SpringerBriefs in Computer Science},
  keywords  = {programming},
  language  = {en}
}

@ARTICLE{Miyagawa2023,
  title        = {The ratio of circulating eicosapentaenoic acid to arachidonic
                  acid ratio in the community-dwelling Japanese population},
  author       = {Miyagawa, Naoko},
  journaltitle = {J. Atheroscler. Thromb.},
  publisher    = {Japan Atherosclerosis Society},
  volume       = {30},
  issue        = {6},
  pages        = {587--588},
  date         = {2023-06-01},
  abstract     = {The Ratio of Circulating Eicosapentaenoic Acid to Arachidonic
                  Acid Ratio in the Community-Dwelling Japanese Population},
  urldate      = {2024-04-29},
  note         = {2002},
  keywords     = {programming},
  language     = {en}
}

@ARTICLE{Nelson2017,
  title        = {Potential benefits of eicosapentaenoic acid on atherosclerotic
                  plaques},
  author       = {Nelson, J R and Wani, O and May, H T and Budoff, M},
  journaltitle = {Vascul. Pharmacol.},
  publisher    = {Vascul Pharmacol},
  volume       = {91},
  pages        = {1--9},
  date         = {2017-04},
  abstract     = {Residual cardiovascular (CV) risk remains in some patients
                  despite optimized statin therapy and may necessitate add-on
                  therapy to reduce this risk. Eicosapentaenoic acid (EPA), an
                  omega-3 polyunsaturated fatty acid, lowers plasma triglyceride
                  levels without raising low-density lipoprotein cholesterol
                  levels and has potential beneficial effects on atherosclerotic
                  plaques. Animal studies have shown that EPA reduces levels of
                  pro-inflammatory cytokines and chemokines. In clinical trials
                  utilizing a wide spectrum of plaque imaging modalities, EPA
                  has shown beneficial effects on plaque characteristics.
                  Studies of patients with coronary artery disease receiving
                  statin therapy suggest that EPA may decrease plaque
                  vulnerability and prevent plaque progression. EPA also
                  decreased pentraxin-3 and macrophage accumulation. A large,
                  randomized, Japanese study reported that EPA plus a statin
                  resulted in a 19\% relative reduction in major coronary events
                  at 5years versus a statin alone in patients with
                  hypercholesterolemia (P=0.011). Icosapent ethyl, a high-purity
                  prescription form of EPA ethyl ester, has been shown to reduce
                  triglyceride levels and markers of atherosclerotic
                  inflammation. Results of an ongoing CV outcomes study will
                  further define the potential clinical benefits of icosapent
                  ethyl in reducing CV risk in high-risk patients receiving
                  statin therapy.},
  urldate      = {2024-04-29},
  keywords     = {Atherosclerosis; Atherosclerotic plaque; Eicosapentaenoic
                  acid; Eicosapentaenoic acid ethyl ester; Omega-3 fatty
                  acids;programming},
  language     = {en}
}

@MISC{Xu2024,
  title       = {Tunnel Try-on: Excavating Spatial-temporal Tunnels for
                 High-quality Virtual Try-on in Videos},
  author      = {Xu, Zhengze and Chen, Mengting and Wang, Zhao and Xing, Linyu
                 and Zhai, Zhonghua and Sang, Nong and Lan, Jinsong and Xiao,
                 Shuai and Gao, Changxin},
  date        = {2024-04-26},
  eprintclass = {cs.CV},
  abstract    = {Video try-on is a challenging task and has not been well
                 tackled in previous works. The main obstacle lies in preserving
                 the details of the clothing and modeling the coherent motions
                 simultaneously. Faced with those difficulties, we address video
                 try-on by proposing a diffusion-based framework named "Tunnel
                 Try-on." The core idea is excavating a "focus tunnel" in the
                 input video that gives close-up shots around the clothing
                 regions. We zoom in on the region in the tunnel to better
                 preserve the fine details of the clothing. To generate coherent
                 motions, we first leverage the Kalman filter to construct
                 smooth crops in the focus tunnel and inject the position
                 embedding of the tunnel into attention layers to improve the
                 continuity of the generated videos. In addition, we develop an
                 environment encoder to extract the context information outside
                 the tunnels as supplementary cues. Equipped with these
                 techniques, Tunnel Try-on keeps the fine details of the
                 clothing and synthesizes stable and smooth videos.
                 Demonstrating significant advancements, Tunnel Try-on could be
                 regarded as the first attempt toward the commercial-level
                 application of virtual try-on in videos.},
  urldate     = {2024-04-29},
  note        = {2024},
  keywords    = {programming}
}

@MISC{Zhang2024,
  title       = {{MaPa}: Text-driven Photorealistic Material Painting for {3D}
                 Shapes},
  author      = {Zhang, Shangzhan and Peng, Sida and Xu, Tao and Yang, Yuanbo
                 and Chen, Tianrun and Xue, Nan and Shen, Yujun and Bao, Hujun
                 and Hu, Ruizhen and Zhou, Xiaowei},
  date        = {2024-04-26},
  eprintclass = {cs.CV},
  abstract    = {This paper aims to generate materials for 3D meshes from text
                 descriptions. Unlike existing methods that synthesize texture
                 maps, we propose to generate segment-wise procedural material
                 graphs as the appearance representation, which supports
                 high-quality rendering and provides substantial flexibility in
                 editing. Instead of relying on extensive paired data, i.e., 3D
                 meshes with material graphs and corresponding text
                 descriptions, to train a material graph generative model, we
                 propose to leverage the pre-trained 2D diffusion model as a
                 bridge to connect the text and material graphs. Specifically,
                 our approach decomposes a shape into a set of segments and
                 designs a segment-controlled diffusion model to synthesize 2D
                 images that are aligned with mesh parts. Based on generated
                 images, we initialize parameters of material graphs and
                 fine-tune them through the differentiable rendering module to
                 produce materials in accordance with the textual description.
                 Extensive experiments demonstrate the superior performance of
                 our framework in photorealism, resolution, and editability over
                 existing methods. Project page:
                 https://zhanghe3z.github.io/MaPa/},
  urldate     = {2024-04-29},
  note        = {2024},
  keywords    = {programming}
}

@MISC{Noman2024,
  title       = {{ChangeBind}: A Hybrid Change Encoder for Remote Sensing Change
                 Detection},
  author      = {Noman, Mubashir and Fiaz, Mustansar and Cholakkal, Hisham},
  date        = {2024-04-26},
  eprintclass = {cs.CV},
  abstract    = {Change detection (CD) is a fundamental task in remote sensing
                 (RS) which aims to detect the semantic changes between the same
                 geographical regions at different time stamps. Existing
                 convolutional neural networks (CNNs) based approaches often
                 struggle to capture long-range dependencies. Whereas recent
                 transformer-based methods are prone to the dominant global
                 representation and may limit their capabilities to capture the
                 subtle change regions due to the complexity of the objects in
                 the scene. To address these limitations, we propose an
                 effective Siamese-based framework to encode the semantic
                 changes occurring in the bi-temporal RS images. The main focus
                 of our design is to introduce a change encoder that leverages
                 local and global feature representations to capture both subtle
                 and large change feature information from multi-scale features
                 to precisely estimate the change regions. Our experimental
                 study on two challenging CD datasets reveals the merits of our
                 approach and obtains state-of-the-art performance.},
  urldate     = {2024-04-29},
  note        = {2024},
  keywords    = {programming}
}

@MISC{Nam2024,
  title       = {An exactly solvable model for emergence and scaling laws},
  author      = {Nam, Yoonsoo and Fonseca, Nayara and Lee, Seok Hyeong and
                 Louis, Ard},
  date        = {2024-04-26},
  eprintclass = {cs.LG},
  abstract    = {Deep learning models can exhibit what appears to be a sudden
                 ability to solve a new problem as training time ($T$), training
                 data ($D$), or model size ($N$) increases, a phenomenon known
                 as emergence. In this paper, we present a framework where each
                 new ability (a skill) is represented as a basis function. We
                 solve a simple multi-linear model in this skill-basis, finding
                 analytic expressions for the emergence of new skills, as well
                 as for scaling laws of the loss with training time, data size,
                 model size, and optimal compute ($C$). We compare our detailed
                 calculations to direct simulations of a two-layer neural
                 network trained on multitask sparse parity, where the tasks in
                 the dataset are distributed according to a power-law. Our
                 simple model captures, using a single fit parameter, the
                 sigmoidal emergence of multiple new skills as training time,
                 data size or model size increases in the neural network.},
  urldate     = {2024-04-29},
  note        = {2024},
  keywords    = {programming}
}

@MISC{Lin2024,
  title       = {A Novel Context driven Critical Integrative Levels ({CIL})
                 Approach: Advancing Human-Centric and Integrative Lighting
                 Asset Management in Public Libraries with Practical Thresholds},
  author      = {Lin, Jing and Mylly, Nina and Hedekvist, Per Olof and Shen,
                 Jingchun},
  date        = {2024-04-26},
  eprintclass = {cs.HC},
  abstract    = {This paper proposes the context driven Critical Integrative
                 Levels (CIL), a novel approach to lighting asset management in
                 public libraries that aligns with the transformative vision of
                 human-centric and integrative lighting. This approach
                 encompasses not only the visual aspects of lighting performance
                 but also prioritizes the physiological and psychological
                 well-being of library users. Incorporating a newly defined
                 metric, Mean Time of Exposure (MTOE), the approach quantifies
                 user-light interaction, enabling tailored lighting strategies
                 that respond to diverse activities and needs in library spaces.
                 Case studies demonstrate how the CIL matrix can be practically
                 applied, offering significant improvements over conventional
                 methods by focusing on optimized user experiences from both
                 visual impacts and non-visual effects.},
  urldate     = {2024-04-29},
  note        = {2024},
  keywords    = {programming}
}

@MISC{temporaryCitekey-ogtpw,
  title       = {Federated Transfer Component Analysis Towards Effective {VNF}
                 Profiling},
  author      = {ZhangB, Xunzheng and Moazzeni, Shadi and Parra-Ullauri, Juan
                 Marcelo and Nejabati, Reza and Simeonidou, Dimitra},
  date        = {2024-04-26},
  eprintclass = {cs.DC},
  abstract    = {The increasing concerns of knowledge transfer and data privacy
                 challenge the traditional gather-and-analyse paradigm in
                 networks. Specifically, the intelligent orchestration of
                 Virtual Network Functions (VNFs) requires understanding and
                 profiling the resource consumption. However, profiling all
                 kinds of VNFs is time-consuming. It is important to consider
                 transferring the well-profiled VNF knowledge to other
                 lack-profiled VNF types while keeping data private. To this
                 end, this paper proposes a Federated Transfer Component
                 Analysis (FTCA) method between the source and target VNFs. FTCA
                 first trains Generative Adversarial Networks (GANs) based on
                 the source VNF profiling data, and the trained GANs model is
                 sent to the target VNF domain. Then, FTCA realizes federated
                 domain adaptation by using the generated source VNF data and
                 less target VNF profiling data, while keeping the raw data
                 locally. Experiments show that the proposed FTCA can
                 effectively predict the required resources for the target VNF.
                 Specifically, the RMSE index of the regression model decreases
                 by 38.5\% and the R-squared metric advances up to 68.6\%.},
  urldate     = {2024-04-29},
  note        = {2024},
  keywords    = {programming}
}

@MISC{temporaryCitekey-pvypt,
  title       = {{CoCar} {NextGen}: a Multi-Purpose Platform for Connected
                 Autonomous Driving Research},
  author      = {Heinrich, Marc and Zipfl, Maximilian and Uecker, Marc and Ochs,
                 Sven and Gontscharow, Martin and Fleck, Tobias and Doll, Jens
                 and Schörner, Philip and Hubschneider, Christian and Zofka,
                 Marc René and Viehl, Alexander and Zöllner, J Marius},
  date        = {2024-04-26},
  eprintclass = {cs.RO},
  abstract    = {Real world testing is of vital importance to the success of
                 automated driving. While many players in the business design
                 purpose build testing vehicles, we designed and build a modular
                 platform that offers high flexibility for any kind of scenario.
                 CoCar NextGen is equipped with next generation hardware that
                 addresses all future use cases. Its extensive, redundant sensor
                 setup allows to develop cross-domain data driven approaches
                 that manage the transfer to other sensor setups. Together with
                 the possibility of being deployed on public roads, this creates
                 a unique research platform that supports the road to automated
                 driving on SAE Level 5.},
  urldate     = {2024-04-29},
  note        = {2024},
  keywords    = {programming}
}
